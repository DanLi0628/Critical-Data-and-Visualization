After listening to the podcasts and watching the videos, I learned that the unevenness and discrimination existing in social systems are not from manual operations, but from machine learning. Machine learning refers to the process that computers use plenty of data to improve its algorithm, which seems to be neutral but in fact is biased. All the data and the way to use data are the products of human history and human choices. Therefore, the algorithm will inherit and even enhance such discrimination.
Since the discrimination exists in social system and many people still believe that the system is flawless, when the system makes some mistakes because of the biases, people will not acknowledge such mistake. In other words, the discrimination is disguised in a harmless form. For example, a black woman is requesting a loan or insurance for an emergency. But because of her identity of “black” and “woman,” she is more likely to be discriminated against compared to other people like white males. Thus, the black woman may not get the money for her emergency, and she can argue with nobody. The algorithm cannot communicate, while people believe in the algorithm. She is trapped in this “electronic incarceration” and falls victim to the system.
There are several cases in China as well. Chinese people are either used to, or not aware of the biases in social systems and the existence of the government’s surveillance. One example is the credit system. Buying a train ticket, requesting for a loan, finding a job and many other things are related to the credit system. However, such “credit” system cannot imply the person’s financial condition, nor can it indicate whether the person is creditable. Another case is the monitors that are all around us nowadays. It has happened several times that the police caught the criminal at a concert thanks to the facial recognition technology. It indicates the fact that everyone’s features and history are captured and recorded in the system. The result that the police caught the criminal sounds good, but to think from another angle, who can have access to our records? And what will these people do with our data? In many cases, we, who provide the data, are thinking from the people in power’s perspective and believe their words that “the technique is for a greater good,” which they cannot achieve in the end. 
